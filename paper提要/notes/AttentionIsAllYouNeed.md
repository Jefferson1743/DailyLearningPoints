# attention is all you need
## 摘要

有别于传统的RNN的EncoderDecoder结构，paper提出一种基于纯attention的模型，舍弃卷积和循环结构。
## 结论

在EncoderDecoder结构中使用注意力机制，替换原有的卷积层和循环层。在两个任务上表现较以往的工作有提升。

## 引言

引言部分的对RNN在处理时序数据时的特点，以来上一个时刻的状态进行计算，这严重限制了模型的并行性，同时需要存储大量的隐含状态，需要占用内存进行存储。

attention机制，不需要时序位置信息，也能够捕获全局的对应关系。

